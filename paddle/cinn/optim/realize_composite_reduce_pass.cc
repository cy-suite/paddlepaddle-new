// Copyright (c) 2025 PaddlePaddle Authors. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#include "paddle/cinn/optim/realize_composite_reduce_pass.h"
#include "paddle/cinn/hlir/pe/reduction.h"
#include "paddle/cinn/ir/ir_printer.h"
#include "paddle/cinn/ir/ir_mutator.h"
#include "paddle/cinn/ir/stmt_visitors.h"
#include "paddle/cinn/ir/utils/ir_copy.h"
#include "paddle/phi/core/enforce.h"

namespace cinn {
namespace optim {

using ir::stmt::Alloc;
using ir::stmt::BlockRef;
using ir::stmt::Evaluate;
using ir::stmt::For;
using ir::stmt::Free;
using ir::stmt::IfThenElse;
using ir::stmt::Let;
using ir::stmt::Schedule;
using ir::stmt::StmtRef;
using ir::stmt::Store;

namespace {

enum CompositeReduceType {
  None = 0x0,
  Welford = 0x1,
  ArgIdx  = 0x2,         // index type i32
  NumReduceType
};

constexpr std::array<const char*, NumReduceType> REDUCE_TYPE_NAMES_ = {
  "None", "Welford Variance", "Arg Reduce"
};

CompositeReduceType GetReduceType(const ir::Expr& expr) {
  if (auto it = expr.As<ir::Call>()) {
    if (it->name == hlir::pe::kVarianceFuncName) {
      return CompositeReduceType::Welford;
    } else if (it->name == hlir::pe::kArgMaxFuncName ||
               it->name == hlir::pe::kArgMinFuncName
    ) {
      return CompositeReduceType::ArgIdx;
    }
  }
  return CompositeReduceType::None;
}

struct CompositeTypes: public std::vector<common::Type> {
  CompositeReduceType type = None;
  CompositeTypes(CompositeReduceType _type = None): type(_type) {
    this->reserve(2);
  }

  void print() const {
    VLOG(4) << "[CompositeTypes]: ";
    for (auto _t: *this) {
      VLOG(4) << _t;
    }
  }
};

CompositeTypes GetArgReduceUnderlyingType(const ir::Expr& expr) {
  if (auto it = expr.As<ir::Call>()) {
    if (it->name == hlir::pe::kArgMaxFuncName ||
        it->name == hlir::pe::kArgMinFuncName
    ) {
      // for cinn_argxxx func, the arg1 is the argidx
      // we need to check the type of the input
      auto argidx_call = it->read_args[1].As<ir::Call>();
      if (argidx_call != nullptr && 
          argidx_call->name.find("argidx_") == 0
      ) {
        CompositeTypes comp_types(ArgIdx);
        comp_types.push_back(argidx_call->read_args[0]->type());
        comp_types.push_back(expr->type());
        return comp_types;
      }
    } else if (it->name == hlir::pe::kVarianceFuncName) {
      return CompositeTypes(Welford);
    }
  }
  return CompositeTypes();
}

void SetInitValue(
  Store store_stmt, 
  common::Type new_type,
  const CompositeTypes& comp_type
) {
  ir::Expr init_value = store_stmt->value();
  if (comp_type.type == CompositeReduceType::Welford) {
    store_stmt->set_value(
        ir::Call::Make(new_type,
                      new_type.customized_type(),
                      {init_value, init_value, init_value},
                      {},
                      ir::CallType::Intrinsic)
    );
  } else if (comp_type.type == CompositeReduceType::ArgIdx) {
    ir::Expr index_init = ir::Expr(INT_MAX);
    index_init->set_type(common::Int(32));
    if (comp_type.at(1).is_int(64)) {
      index_init = ir::Expr(INT64_MAX);
      index_init->set_type(common::Int(64));
    }
    store_stmt->set_value(
        ir::Call::Make(new_type,
                      new_type.customized_type(),
                      {init_value, index_init},
                      {},
                      ir::CallType::Intrinsic)
    );
  } else {
    PADDLE_THROW(
        ::common::errors::Unimplemented("reduce_type '%s' not allowed.",
          REDUCE_TYPE_NAMES_[(int)comp_type.type])
    );
  }
}

template <typename StmtType>
std::set<ir::Buffer> CollectReduceBuffers(const BlockRef& body) {
  std::set<ir::Buffer> buffers;

  const auto VisitFn = [&](const StmtRef& stmt) {
    if (!stmt.isa<StmtType>()) {
      // VLOG(4) << "[!] stmt:" << stmt << ", type is: " << stmt->type();
      return;
    }
    // VLOG(4) << "[yes] stmt:" << stmt << ", type is: " << stmt->type();
    StmtType store_stmt = stmt.as<StmtType>();
    if (GetReduceType(store_stmt->value()) != 
        CompositeReduceType::None) {
      buffers.insert(store_stmt->tensor().as_tensor()->buffer);
    }
  };

  ir::stmt::Visit(body, VisitFn, [](auto) {});
  return buffers;
}

CompositeTypes DetermineReduceType(const BlockRef& body) {
  CompositeTypes composite_reduce;

  // the blockref can only have one unique reduce type
  const auto VisitFn = [&composite_reduce](const StmtRef& stmt) {
    // to stop recursive visit: (1) rtype is defined
    // (2) stmt is not a Store stmt
    if (composite_reduce.type != CompositeReduceType::None || 
        !stmt.isa<Store>()
    ) return;
    Store store_stmt = stmt.as<Store>();
    composite_reduce = GetArgReduceUnderlyingType(store_stmt->value());
  };

  ir::stmt::Visit(body, VisitFn, [](auto) {});
  return composite_reduce;
}

Store GetStoreOfSchedule(const Schedule& stmt) {
  Store store_stmt;
  bool found = false;
  const auto VisitFn = [&](StmtRef stmt) {
    if (!found && stmt.isa<Store>()) {
      store_stmt = stmt.as<Store>();
      found = true;
    }
  };
  ir::stmt::Visit(stmt->body(), VisitFn, [](auto) {});
  PADDLE_ENFORCE(found,
                 ::common::errors::PreconditionNotMet(
                     "One Schedule should have exactly one Store."));
  return store_stmt;
}

Type GetCompositeReduceType(
  const Type& elem_type,
  const CompositeTypes& composite_reduce
) {
  int type_bits = 0;
  std::string rtype_name = "";
  switch (composite_reduce.type) {
    case CompositeReduceType::Welford: {
      type_bits = elem_type.bits() * 3;
      rtype_name = "welford" + 
                hlir::pe::Type2StrForReduce(elem_type);
      break;
    }
    case CompositeReduceType::ArgIdx: {
      PADDLE_ENFORCE_GT(composite_reduce.size(), 1, 
        ::common::errors::InvalidArgument("CompositeTypes for arg reduce "
                                          "must have at least two types"
      ));
      int max_bits = std::max(composite_reduce[0].bits(), composite_reduce[1].bits());
      type_bits = max_bits * 2;
      rtype_name = "argidx" + 
                hlir::pe::Type2StrForArgReduce(composite_reduce[0]) +
                hlir::pe::Type2StrForArgReduce(composite_reduce[1]);
      break;
    }
  default:
    PADDLE_THROW(
      ::common::errors::InvalidArgument("Unsupported composite reduce type: %s", 
        REDUCE_TYPE_NAMES_[(int)composite_reduce.type]));
  }
  Type customized_type(ir::Type::type_t::Customized,
                    /* bits = */ type_bits,
                    /* width = */ 1);
  customized_type.set_customized_type(rtype_name);
  customized_type.set_cpp_const(false);
  return customized_type;
}

struct StageReduceResultMutator : public ir::stmt::StmtMutator<> {
  explicit StageReduceResultMutator(ir::LoweredFunc func) : func_(func) {
    for (auto& arg : func->args) {
      if (arg.is_buffer()) arg_buffers_.insert(arg.buffer_arg());
    }
  }

  void operator()(BlockRef block) { VisitBlock(block); }

 private:
  void VisitStmt(Schedule stmt) override {
    if (stmt->name().substr(0, 4) == "root") {
      ir::stmt::StmtMutator<>::VisitBlock(stmt->body());
      return;
    }
    Store store_stmt = GetStoreOfSchedule(stmt.as<Schedule>());
    auto* store_tensor = store_stmt->tensor().as_tensor();
    if (GetReduceType(store_stmt->value()) == CompositeReduceType::None) return;
    if (arg_buffers_.count(store_tensor->buffer) == 0) return;

    // Create the staging buffer.
    // We only need one element for this buffer, so its shape is {1}.
    const std::vector<ir::Expr> shape = {ir::Expr(1)};
    const std::vector<ir::Expr> indices = {ir::Expr(0)};
    ir::Tensor staging_tensor =
        ir::_Tensor_::Make(common::UniqName(store_tensor->name + "_local"),
                           store_tensor->buffer->dtype,
                           shape,
                           shape);
    staging_tensor->WithBuffer("local", staging_tensor->name + "_buffer");
    func_->temp_bufs.push_back(staging_tensor->buffer);

    // Create the staging Schedule.
    Schedule staging_schedule(stmt->iter_vars(),
                              stmt->iter_values(),
                              stmt->read_buffers(),
                              stmt->write_buffers(),
                              staging_tensor->name,
                              ir::ir_utils::IRCopy(stmt->body()),
                              stmt->attrs(),
                              stmt->reduce_method());
    sibling_stmts_.push_back(staging_schedule);

    // Replace all uses of the composite reduce buffer with the staging buffer.
    Store staging_store = GetStoreOfSchedule(staging_schedule);
    staging_store->set_tensor(staging_tensor);
    staging_store->set_indices(indices);
    ir::Expr staging_value = staging_store->value();
    staging_value.As<ir::Call>()->read_args[0] =
        ir::Load::Make(staging_tensor, indices);
    staging_store->set_value(staging_value);
    store_stmt->set_value(ir::Load::Make(staging_tensor, indices));

    // Remove the reduction flags in the current Schedule, because reduction
    // has been done in the staging Schedule.
    std::vector<ir::Var> new_iter_vars;
    for (auto& var : stmt->iter_vars()) {
      ir::Var new_var = var->Copy().as_var_ref();
      new_var->is_reduce_axis = false;
      new_iter_vars.push_back(new_var);
    }
    stmt->set_iter_vars(new_iter_vars);
  }

  void VisitBlock(BlockRef block) override {
    std::vector<StmtRef> old_stmts;
    old_stmts.swap(sibling_stmts_);

    for (StmtRef stmt : block->stmts()) {
      ir::stmt::StmtMutator<>::VisitStmt(stmt);
      sibling_stmts_.push_back(stmt);
    }

    block->set_stmts(sibling_stmts_);
    sibling_stmts_ = std::move(old_stmts);
  }

  void VisitStmt(For stmt) override { VisitBlock(stmt->body()); }

  void VisitStmt(IfThenElse stmt) override {
    ir::stmt::BlockRef true_case = stmt->true_case();
    VisitBlock(true_case);
    stmt->set_true_case(true_case);
    if (stmt->false_case().defined()) {
      ir::stmt::BlockRef false_case = stmt->false_case();
      VisitBlock(false_case);
      stmt->set_false_case(false_case);
    }
  }

  void VisitStmt(Let stmt) override { return; }
  void VisitStmt(Store stmt) override { return; }
  void VisitStmt(Alloc stmt) override { return; }
  void VisitStmt(Free stmt) override { return; }
  void VisitStmt(Evaluate stmt) override { return; }

 private:
  ir::LoweredFunc func_;
  // buffers in the function's argument list
  std::set<ir::Buffer> arg_buffers_;
  // stmts at the same level with the currently visiting stmt
  std::vector<StmtRef> sibling_stmts_;
};

struct LoadTypeMutator : public ir::IRMutator<> {
  explicit LoadTypeMutator(
      const std::map<ir::Buffer, ir::Type>& buffer2type, 
      const CompositeTypes& ctype)
      : buffer2type_(buffer2type), comp_type_(ctype) {}

  void operator()(ir::Expr* expr) { ir::IRMutator<>::Visit(expr, expr); }

 private:
  void Visit(const ir::Load* op, ir::Expr* expr) override {
    ir::IRMutator<>::Visit(op, expr);
    auto* node = expr->As<ir::Load>();
    auto& buffer = node->tensor.as_tensor()->buffer;
    // VLOG(4) << "[Customized reduce] old type is: " << node->tensor.as_tensor()->type() << ", tensor: " << node->tensor.as_tensor()->name;
    auto it = buffer2type_.find(buffer);
    if (it != buffer2type_.end()) {
      
      ir::Type new_type = GetCompositeReduceType(it->second, comp_type_);
      node->tensor.as_tensor()->set_type(new_type);
      buffer->dtype = new_type;
      *expr = ir::Cast::Make(it->second, *expr);
    }
  }

  void UncastType(ir::Expr* expr) {
    auto* cast_node = expr->As<ir::Cast>();
    if (!cast_node) return;
    auto* load_node = cast_node->v().As<ir::Load>();
    if (!load_node) return;
    if (buffer2type_.count(load_node->tensor.as_tensor()->buffer) > 0) {
      *expr = cast_node->v();
    }
  }

  void Visit(const ir::Call* op, ir::Expr* expr) override {
    // this function will cast the buffer from composite type
    // to an underlying type, for example welford_fp32 -> float
    // uncast will undo this process
    ir::IRMutator<>::Visit(op, expr);
    // By default, all tensors are casted back to their element type
    // before doing other computation. However, for the composite reduction 
    // calls, we shouldn't cast the arguments back because they hold the 
    // intermediate status.
    if (GetReduceType(*expr) != CompositeReduceType::None) {
      auto* node = expr->As<ir::Call>();
      UncastType(&(node->read_args[0]));
      UncastType(&(node->read_args[1]));
    }
  }

  const std::map<ir::Buffer, ir::Type>& buffer2type_;
  const CompositeTypes& comp_type_;
};

void SetBufferType(ir::LoweredFunc func,
                   const std::set<ir::Buffer>& buffers,
                   const CompositeTypes& composite_type
                   ) {
  // Make a map from the buffers to their element types, otherwise it's hard to
  // know a buffer's original type.

  std::map<ir::Buffer, ir::Type> buffer2type;
  for (auto& buffer : buffers) {
    buffer2type.emplace(buffer, buffer->dtype);
  }

  // Set function's temp_bufs type
  for (auto& buffer : func->temp_bufs) {
    auto it = buffer2type.find(buffer);
    if (it != buffer2type.end()) {
      buffer->dtype = GetCompositeReduceType(it->second, composite_type);
    }
  }

  const auto VisitFn = [&](StmtRef stmt) {
    if (!stmt.isa<Store>()) return;
    Store store_stmt = stmt.as<Store>();
    auto* tensor = store_stmt->tensor().as_tensor();
    auto& buffer = tensor->buffer;

    VLOG(4) << "Buffer: " << buffer->name << ", tensor: " << tensor->name;
    // Set store buffer type
    auto it = buffer2type.find(buffer);
    if (it != buffer2type.end()) {
      ir::Expr new_tensor = ir::ir_utils::IRCopy(store_stmt->tensor());
      ir::Type new_type = GetCompositeReduceType(it->second, composite_type);
      new_tensor.as_tensor()->set_type(new_type);
      new_tensor.as_tensor()->buffer->dtype = new_type;
      store_stmt->set_tensor(new_tensor);

      if (ir::IsReduceInitTensorName(tensor->name)) {
        SetInitValue(store_stmt, new_type, composite_type);
      }
    }

    // Set load buffer type
    ir::Expr new_value = ir::ir_utils::IRCopy(store_stmt->value());
    LoadTypeMutator load_type_mutator(buffer2type, composite_type);
    load_type_mutator(&new_value);
    store_stmt->set_value(new_value);
  };

  ir::stmt::Mutate(func->body_block, VisitFn, [](auto) {});
}

struct ReduceExternCallMutator : public ir::IRMutator<> {
  void operator()(ir::Expr* expr) { ir::IRMutator<>::Visit(expr, expr); }

 private:
  void Visit(const ir::Call* op, ir::Expr* expr) override {
    ir::IRMutator<>::Visit(op, expr);
    auto reduce_type_ = GetReduceType(*expr);
    if (reduce_type_ == CompositeReduceType::None) return;
    ir::Expr lhs = op->read_args[0];
    ir::Expr rhs = op->read_args[1];
    if (lhs.type() != rhs.type()) {
      if (auto call_op = rhs.As<ir::Call>()) {
        // for argidx type, avoid redundant type casting, but this is ugly
        if (call_op->name.find("argidx") != std::string::npos) {
          rhs->set_type(lhs.type());
        }
      } else {
        rhs = ir::Cast::Make(lhs.type(), rhs);
      }
    }
    if (reduce_type_ == CompositeReduceType::Welford) {
      // replace cinn_reduce_variance to operator+
      *expr = ir::Add::Make(lhs, rhs);
    } else if (reduce_type_ == CompositeReduceType::ArgIdx) {
      // replace cinn_argmxx_iyy to max or min (overloaded) 
      if (op->name.find("argmax") != std::string::npos) {
        *expr = ir::Max::Make(lhs, rhs);
      } else {
        *expr = ir::Min::Make(lhs, rhs);
      }
    }
  }
};

void ReplaceReduceExternCall(const BlockRef& body) {
  const auto VisitFn = [&](StmtRef stmt) {
    if (!stmt.isa<Store>()) return;
    Store store_stmt = stmt.as<Store>();
    ir::Expr new_value = ir::ir_utils::IRCopy(store_stmt->value());
    ReduceExternCallMutator()(&new_value);
    store_stmt->set_value(new_value);
  };

  ir::stmt::Mutate(body, VisitFn, [](auto) {});
}

}  // namespace

LogicalResult RealizeCompositeReducePass::Run(ir::LoweredFunc func) {
  VLOG(4) << "In function RealizeCompositeReducePass::Run";
  BlockRef body = func->body_block;

  // Step 1. Create a staging buffer for composite reduction result if it is
  //   directly written to the function's argument. This is because the
  //   result and the argument have different data types, and we need a staging
  //   buffer to do casting properly.
  // Note: theoretically, we don't need this mutator if all reduction results
  //   are explicitly written back to global memory by yield_stores. However,
  //   current CINN frontend cannot guarantee this, so we need to do staging by
  //   ourself if the expected yield_store is missing.
  StageReduceResultMutator mutator(func);
  VLOG(4) << "\t[CompositeReducePass] Staging Reduce mutator, starts...";
  mutator(body);
  VLOG(4) << "\t[CompositeReducePass] Staging Reduce mutator, completed. Reduce body: " << body;

  auto composite_type = DetermineReduceType(body);
  if (composite_type.type == CompositeReduceType::None) {
    return LogicalResult::success();
  }
  composite_type.print();
  
  // Step 2. Collect buffers that are used for reduce computation.
  std::set<ir::Buffer> buffers = CollectReduceBuffers<Store>(body);

  VLOG(4) << "\t[CompositeReducePass] Reduce Buffer collected.";

  // Step 3. Change the data type of buffers to the corresponding type.
  SetBufferType(func, buffers, composite_type);

  VLOG(4) << "\t[CompositeReducePass] Buffer type set." << body;

  // Step 4. Replace the `cinn_reduce_variance` calls to `operator+` in order to
  //   reuse the cross-thread/block reduction templates.
  ReplaceReduceExternCall(body);

  VLOG(4) << "\t[CompositeReducePass] ExternCalls replaced.";

  return LogicalResult::success();
}

std::unique_ptr<FuncPass> CreateRealizeCompositeReducePass() {
  return std::make_unique<RealizeCompositeReducePass>();
}

}  // namespace optim
}  // namespace cinn
